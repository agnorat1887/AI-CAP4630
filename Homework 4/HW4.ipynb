{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eL3OHoodYwH",
        "colab_type": "text"
      },
      "source": [
        "#General Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUluwFlVda99",
        "colab_type": "text"
      },
      "source": [
        "Artificial intelligence consists of using a program to allow computer to essentially \"Learn\" and think more like humans do. Humans can look at a wall and determine whats on it, for example we can easily distinguish between the wall and a clock on it. However, computers are not able to make this distinction themselves. AI allows the computer to gaine the \"Intelligence' in order to determine that what is on the wall is indeed a clock. This type of problem can be seen not only as an AI problem, but a machine learning one as well. Machine learning is basically a sub-category of AI. Machine learning is what allows computers to determine what something is without explicitely programming in that this is X and that is Y. Deep learning can then be seen as a sub catgeory or machine learning as deep learning is what allows multi layer neural networks to work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srK00iCKfGVc",
        "colab_type": "text"
      },
      "source": [
        "#Building a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4Tpl9pZfH-Y",
        "colab_type": "text"
      },
      "source": [
        "A convolutional nerural network (Convnet/CNN) is especially useful for classifying unstructured data for object detection and image classification. \n",
        "\n",
        "CNNâ€™s differ from regular neural networks in that their weights consist of small filters, also called kernels, that extract features from the data input. These filters slide over the input image, performing convolutions. Every convolutional output is known as an activation. And the entire matrix is known as a feature map. The CNN uses Gradient Descent to determine the optimal values in each filter, as weighted by the user. To extract additional features all that must be done is to apply more filters, resulting in a two-dimensional feature map being used to create a multi-channel output. Each channel of the multi-channel output represents a different feature, and this whole process makes up a single convolutional layer in the convolutional neural networks. \n",
        "\n",
        "An example of creating a CNN is show in the code snippet below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88HZmSg8gYIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    model = models.Sequential()\n",
        "    model.add(conv_base2)\n",
        "    model.add(layers.GlobalAveragePooling2D())\n",
        "    model.add(layers.Dense(fcLayer1, activation='relu'))\n",
        "    model.add(layers.Dropout(dropout))\n",
        "    model.add(layers.Dense(classes, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FrmdLPlgw36",
        "colab_type": "text"
      },
      "source": [
        "Between every convolution layer there is a pooling layer. The pooling layer is a window sliding over sections of a matrix, applying a function. The purpose of a pooling layer is to downsample data so it is easier to process. The pooling layer will take the input data passed through it and compress it mathematically,such that what are normally visual patterns and features become numerical representations that take up less space.\n",
        "\n",
        "The ReLU function works by replacing all negative values in a feature map with the value zero. ReLU as a nonlinearity is used almost universally. The reason it is so popular is because it just so happens to work the most effectively, pretty much across the board. In practice, every convolutional layer in a CNN model will be followed by a ReLU with the exception of the final convolution.\n",
        "\n",
        "The final convolutional layer will be something like softmax or Sigmoid function, as these are able to create an actual output for our predictions and classify the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqu9xEYmoZZf",
        "colab_type": "text"
      },
      "source": [
        "#Compiling a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oENmFZ77odWb",
        "colab_type": "text"
      },
      "source": [
        "When compiling a model we can use optimizers such as gradient descent so that it can tie the loss function and model parameters together by updating the model. It will essentially mess with the weights so that you can get the most accurate model, or at least a more accurate model than with a pre-set weight. Since we cant possibly know what the weights should be from the start, the optimizer is able to adjust the weights as it goes so that we can get a bettet weight as the model trains.\n",
        "\n",
        "The learning rate is essentially how much the weights will change in response to the optimzer. By setting a learning rate too big, we will likely pass the value that is the optimal rate, but if its too small, we will take too long in order to reach the value and we could even get stuck in a local minimum because we arent able to \"Hop\" out of it because our learning rate is so low.\n",
        "\n",
        "A loss function is a way to see how well the algorithm is able to interpret the dataset that it was given. The lower value for the loss, the better. A basic loss function would take something like our predicted value from the algorithm and subtract it with the ground truth value of the image that was input. The smaller the difference is, smaller the loss, and therefore the more accurate the algorithm is.\n",
        "\n",
        "The code snippet below shows a model that makes use of the RMSprop optimizer for its model along with the categorical crossentropy loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr4LR7qiA_cS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=learning_rate, decay=lr_decay), metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqpWiNisDyQs",
        "colab_type": "text"
      },
      "source": [
        "#Training a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggq0CipID20P",
        "colab_type": "text"
      },
      "source": [
        "When training a model, there are two main ways to evaluate the model which is called overfitting and underfitting. If a model underfits, then the test accuracy will be very low. This essentually means that the alogrithm isnt trained enough and when given an image it is unable to give an accurate value. If a model overfits, then the test accuracy will be lower than the training accuracy. This happens when the model trains too much on the training set and its able to recognize specific nuances in the training set that cant translate over to the real world test set.\n",
        "\n",
        "In order to fight overfitting, you can use dropout. Dropout ignores random neurons when training so that when you use a large amount of epochs you dont train the model too much on the same set becuase different neurons will be active each time it runs through the image/data. \n",
        "\n",
        "In the code snippet below, the dropout rate is .3, meaning that there is a 30% chance that an individual neuron will be kept leading to a reduced network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPGxeQELG7JH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(.3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rOGTUJpGAdF",
        "colab_type": "text"
      },
      "source": [
        "#Finetuning a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1Qls25RGDOq",
        "colab_type": "text"
      },
      "source": [
        "You can finetune a model by editting the hyperparameters to the point where the model doesnt underfit or overfit. These hyperparameters are values that are set before training and are values that do not get updated as the learning process runs for the model. \n",
        "\n",
        "You can also unfreeze the model so that you can continue training an existing model so that you can get values that are more accurate or so that you can even start training on a new dataset or an expanded one. You are then able to freeze the model so that the weights no longer change in the model and that is able to completely bypass the backward pass which can help to give training a large speedboost. You need to be careful with freezing the model however as if you freeze to early you can get inaccurate predictions. \n",
        "\n",
        "You can also use data augmentation techniques in order to help train models. You can add noise to images to help the model train for less than ideal conditions. You can also \"Squish\" the image so that the image looks distorted but not so bad that you cant tell what is in the image. You can also add masks to change the image as well as change the bounding boxes so that the algorithm gets more or less information than it would have gotten with the original image. Using data augmentation, you can take 1 image, and use it multiple times to train the model with less of a change of it overfitting since you arent using the same image multiple times and helping the model to recoginize a large set of data and makes it more flexible. "
      ]
    }
  ]
}